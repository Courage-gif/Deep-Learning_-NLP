{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZMxB0oqFRpk4Rmr+PGQPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Courage-gif/Deep-Learning_-NLP/blob/main/Document_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Courage Laizan\n",
        "\n",
        "R227552Q"
      ],
      "metadata": {
        "id": "NprIadteY7r7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link To the summary  document - https://docs.google.com/document/d/15XS-3q4zGqqyM9SnpCkR9LCLJkiuzZAg/edit?usp=sharing&ouid=116110048755074416183&rtpof=true&sd=true"
      ],
      "metadata": {
        "id": "vhUwd0BXyE7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTALL LIBRARIES (FAST)\n",
        "# ============================================\n",
        "!pip install -q pdfminer.six bertopic transformers sentence-transformers \\\n",
        "               scikit-learn gensim umap-learn hdbscan python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qWxbqwBzqrm",
        "outputId": "3894602c-3a10-4c0f-dbaf-a052b692b964"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m265.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agylQ24jxg5h"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GLOBAL DOCUMENT SUMMARY\n",
        "# ============================================\n",
        "import os\n",
        "import warnings\n",
        "from pdfminer.high_level import extract_text\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from docx import Document\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ============================================\n",
        "# INITIAL SETUP\n",
        "# ============================================\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Configuration\n",
        "PDF_FILE_PATH = \"/content/drive/MyDrive/merged_documents.pdf\"\n",
        "\n",
        "# GPU Configuration\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# ============================================\n",
        "# DATA EXTRACTION\n",
        "# ============================================\n",
        "print(\"ğŸ“– Extracting document content...\")\n",
        "full_text = extract_text(PDF_FILE_PATH)\n",
        "print(f\"ğŸ“„ Document size: {len(full_text)} characters\")\n",
        "\n",
        "# ============================================\n",
        "# SMART SUMMARIZATION\n",
        "# ============================================\n",
        "print(\"ğŸ§  Generating intelligent summary...\")\n",
        "\n",
        "# Use a powerful summarization model\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\",  # Excellent for document summarization\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Smart chunking for better context\n",
        "def create_smart_chunks(text, max_chunk_size=1024, overlap=100):\n",
        "    \"\"\"Split text into chunks with sentence boundaries\"\"\"\n",
        "    sentences = text.split('. ')\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk + sentence) < max_chunk_size:\n",
        "            current_chunk += sentence + \". \"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Create chunks and summarize\n",
        "chunks = create_smart_chunks(full_text)\n",
        "print(f\"ğŸ“‹ Created {len(chunks)} content chunks\")\n",
        "\n",
        "# Generate summaries for each chunk\n",
        "summaries = []\n",
        "print(\"â³ Processing content...\")\n",
        "\n",
        "for i, chunk in enumerate(chunks[:20]):  # Limit for reasonable processing time\n",
        "    try:\n",
        "        if len(chunk.split()) > 50:  # Only summarize substantial chunks\n",
        "            summary = summarizer(\n",
        "                chunk,\n",
        "                max_length=150,\n",
        "                min_length=60,\n",
        "                do_sample=False,\n",
        "                truncation=True\n",
        "            )[0][\"summary_text\"]\n",
        "            summaries.append(summary)\n",
        "            print(f\"âœ… Processed chunk {i+1}/{min(len(chunks), 20)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Skipped chunk {i+1}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Combine summaries into coherent global summary\n",
        "if summaries:\n",
        "    # Create a final summary of summaries for coherence\n",
        "    combined_text = \" \".join(summaries)\n",
        "\n",
        "    if len(combined_text.split()) > 200:\n",
        "        try:\n",
        "            global_summary = summarizer(\n",
        "                combined_text,\n",
        "                max_length=300,\n",
        "                min_length=150,\n",
        "                do_sample=False\n",
        "            )[0][\"summary_text\"]\n",
        "        except:\n",
        "            global_summary = combined_text\n",
        "    else:\n",
        "        global_summary = combined_text\n",
        "\n",
        "    print(\"ğŸ‰ Summary generation completed!\")\n",
        "else:\n",
        "    global_summary = \"Unable to generate summary from the provided document.\"\n",
        "    print(\"âŒ No summaries generated\")\n",
        "\n",
        "# ============================================\n",
        "# DOCUMENT INSIGHTS\n",
        "# ============================================\n",
        "print(\"ğŸ“Š Generating document insights...\")\n",
        "\n",
        "# Calculate document statistics\n",
        "total_chars = len(full_text)\n",
        "total_words = len(full_text.split())\n",
        "total_sentences = full_text.count('.') + full_text.count('!') + full_text.count('?')\n",
        "avg_sentence_length = total_words / max(total_sentences, 1)\n",
        "\n",
        "# Estimate reading time (average reading speed: 200-250 words per minute)\n",
        "reading_time_minutes = total_words / 200\n",
        "\n",
        "# Document complexity assessment\n",
        "def assess_complexity(text):\n",
        "    words = text.split()\n",
        "    if len(words) < 1000:\n",
        "        return \"Brief Document\"\n",
        "    elif len(words) < 5000:\n",
        "        return \"Medium Length\"\n",
        "    elif len(words) < 15000:\n",
        "        return \"Comprehensive Document\"\n",
        "    else:\n",
        "        return \"Extensive Research Document\"\n",
        "\n",
        "document_complexity = assess_complexity(full_text)\n",
        "\n",
        "# ============================================\n",
        "# CREATE ENGAGING REPORT\n",
        "# ============================================\n",
        "print(\"ğŸ“ Creating engaging summary report...\")\n",
        "\n",
        "doc = Document()\n",
        "doc.add_heading('ğŸ“š Document Intelligence Report', 0)\n",
        "\n",
        "# Executive Overview\n",
        "doc.add_heading('ğŸ¯ Executive Overview', level=1)\n",
        "doc.add_paragraph(\"This report provides an intelligent summary and key insights extracted from your document using advanced AI analysis.\")\n",
        "\n",
        "# Key Statistics\n",
        "doc.add_heading('ğŸ“ˆ Document Statistics', level=1)\n",
        "stats_table = doc.add_table(rows=5, cols=2)\n",
        "stats_table.style = 'Light Shading Accent 1'\n",
        "\n",
        "stats_data = [\n",
        "    [\"Document Size\", f\"{total_words:,} words\"],\n",
        "    [\"Estimated Reading Time\", f\"{reading_time_minutes:.1f} minutes\"],\n",
        "    [\"Document Complexity\", document_complexity],\n",
        "    [\"Sentences Analyzed\", f\"{total_sentences:,}\"],\n",
        "    [\"Average Sentence Length\", f\"{avg_sentence_length:.1f} words\"]\n",
        "]\n",
        "\n",
        "for i, (metric, value) in enumerate(stats_data):\n",
        "    stats_table.cell(i, 0).text = metric\n",
        "    stats_table.cell(i, 1).text = value\n",
        "\n",
        "# Global Summary\n",
        "doc.add_heading('ğŸ§  Summary', level=1)\n",
        "summary_paragraph = doc.add_paragraph()\n",
        "summary_paragraph.add_run(\"Here's what this document is about:\\n\\n\").bold = True\n",
        "summary_paragraph.add_run(global_summary)\n",
        "\n",
        "# Key Highlights Section\n",
        "doc.add_heading('ğŸ’¡ Key Highlights', level=1)\n",
        "\n",
        "highlight_paragraph = doc.add_paragraph()\n",
        "highlight_paragraph.add_run(\"Based on the analysis:\\n\\n\").bold = True\n",
        "\n",
        "# Add dynamic highlights based on content\n",
        "highlights = [\n",
        "    f\"â€¢ This is a {document_complexity.lower()} with substantial content\",\n",
        "    f\"â€¢ The document contains approximately {total_sentences} key points\",\n",
        "    f\"â€¢ Average complexity level based on sentence structure\",\n",
        "    f\"â€¢ Suitable for readers interested in comprehensive analysis\" if total_words > 3000 else \"â€¢ Concise document ideal for quick reading\"\n",
        "]\n",
        "\n",
        "for highlight in highlights:\n",
        "    doc.add_paragraph(highlight)\n",
        "\n",
        "# Reading Recommendations\n",
        "doc.add_heading('ğŸ“– Reading Guide', level=1)\n",
        "guide_paragraph = doc.add_paragraph()\n",
        "guide_paragraph.add_run(\"For optimal understanding:\\n\\n\").bold = True\n",
        "\n",
        "if reading_time_minutes > 10:\n",
        "    guide_text = f\"â€¢ Set aside {reading_time_minutes:.0f} minutes for thorough reading\\nâ€¢ Focus on key sections highlighted in the summary\\nâ€¢ Consider breaking into multiple sessions for better retention\"\n",
        "else:\n",
        "    guide_text = f\"â€¢ Quick read: Approximately {reading_time_minutes:.0f} minutes\\nâ€¢ Ideal for immediate comprehension\\nâ€¢ Suitable for briefing and discussion\"\n",
        "\n",
        "doc.add_paragraph(guide_text)\n",
        "\n",
        "# Save to Google Drive\n",
        "drive_folder = \"/content/drive/MyDrive/DeepLearning_Projects\"\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "OUTPUT_PATH = os.path.join(drive_folder, \"Document_Summary.docx\")\n",
        "doc.save(OUTPUT_PATH)\n",
        "\n",
        "# ============================================\n",
        "# FINAL OUTPUT\n",
        "# ============================================\n",
        "print(f\"\\nğŸ‰ SUCCESS! Global summary report generated!\")\n",
        "print(f\"ğŸ“ Saved to: {OUTPUT_PATH}\")\n",
        "print(f\"ğŸ“Š Document analyzed: {total_words:,} words\")\n",
        "print(f\"â±ï¸  Reading time: {reading_time_minutes:.1f} minutes\")\n",
        "print(f\"ğŸ“‘ Summary length: {len(global_summary.split())} words\")\n",
        "print(f\"ğŸ“‹ Complexity: {document_complexity}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ§  YOUR DOCUMENT IN A NUTSHELL:\")\n",
        "print(\"=\"*60)\n",
        "print(global_summary)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgUub95w1dzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}